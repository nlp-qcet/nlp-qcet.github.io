<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title> Showing Node QEG-w-9  - Taxonomy of Quality Criteria For Evaluations (QCET) Tool</title>
    <style>
        nav a {
            color: #d64161;
            font-size: 3em;
            margin-left: 50px;
            text-decoration: none;
        }
    </style>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65" crossorigin="anonymous">

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-kenU1KFdBIe4zVF0s0G1M5b4hcpxyD9F7jL+jjXkk+Q2h455rYXK/7HAuoJl+0I4" crossorigin="anonymous"></script>

    <script src="../static/js/app.js"></script>

    <link rel= "stylesheet" type= "text/css" href= "../static/css/app.css">
</head>
<body>
    <nav>
        <a href="#">Taxonomy of Quality Criteria For Evaluations (QCET) Tool</a>
    </nav>
    <hr>
    <div class="content">
        
    <h4> Showing Node QEG-w-9 </h4>
    <div class="pad">
    	<h5>Parent</h5>
				<div id="Goodness of outputs relative to a specified external frame of reference (+/- input), Outputs as a whole" class="taxonomy_node border border-dark rounded-5 border-3 google_light_purple_3 taxonomy_search_node taxonomy_search_node_d_3" style="max-width:1500px;">
					<h5><a class="link-dark" href="./QEG-w.html">QEG-w&nbsp;:&nbsp;Goodness of outputs relative to a specified external frame of reference (+/- input), Outputs as a whole</a></h5>

				</div>
			<br /><h5>Node Details</h5>
			<div id="QEG-w-9" class="taxonomy_node border border-dark rounded-5 border-3 google_light_grey_2 taxonomy_search_node taxonomy_search_node_d_4" style="max-width:1500px;">
				<h5><a class="link-dark" href="./QEG-w-9.html">QEG-w-9&nbsp;:&nbsp;Multi-task Performance</a></h5>
				<p>
					<span class='att_key'>Definition:</span><br /><span class='att_value'>A better system produces outputs that obtain higher aggregated scores on a given set of task datasets and metrics </span><br/><span class='att_key'>Attestations:</span><br /><span class='att_value'><a href="https://arxiv.org/abs/2311.01964">Zhou et al., 2023</a> explore data leakage in LLM assessment, evaluating different models on the MMLU benchmark of 57 different tasks that require real-world knowledge and problem-solving abilities.</span><br/><span class='att_key'>Additional notes and information:</span><br /><span class='att_value'>Multi-task benchmarks have become increasingly common in NLP, particularly in LLM evaluation. [QEG-w-10] Multi-task Performance covers any case where aggregated results expressing performance at multiple tasks is reported.</span>
				</p>
			</div>
		
			<br />
			<h5>Children</h5>
			<ul>
				None
			</ul>
		
    </div>

    </div>
</body>
</html>